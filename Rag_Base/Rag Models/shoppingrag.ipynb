{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e672f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyDXs0FVgsdOAVLTKbsQGsPclv_Dq_Ml1V0\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY_1\")\n",
    "print(os.getenv(\"GOOGLE_API_KEY_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d859471e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to access folder: 'D:\\my_code_profile\\group_project\\Verdant\\Verdant\\Rag_Base\\jsonfiles\\shopping'\n",
      "Folder 'D:\\my_code_profile\\group_project\\Verdant\\Verdant\\Rag_Base\\jsonfiles\\shopping' exists. Listing contents...\n",
      "Loaded 3 pages\n"
     ]
    }
   ],
   "source": [
    "#print(openai_api_key)\n",
    "\n",
    "folder = r\"D:\\my_code_profile\\group_project\\Verdant\\Verdant\\Rag_Base\\jsonfiles\\shopping\"\n",
    "documents = []\n",
    "\n",
    "print(f\"Attempting to access folder: '{folder}'\") # Add this line\n",
    "if not os.path.exists(folder): # Add this check\n",
    "    print(f\"Error: Folder '{folder}' does not exist.\")\n",
    "else:\n",
    "    print(f\"Folder '{folder}' exists. Listing contents...\")\n",
    "\n",
    "for file in os.listdir(folder):\n",
    "    if file.endswith(\".json\"):\n",
    "        f = os.path.join(folder, file)\n",
    "        loader= JSONLoader(file_path=f, jq_schema = \".\", text_content=False )  \n",
    "        documents.extend(loader.load())\n",
    "\n",
    "print(f\"Loaded {len(documents)} pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2d1dceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created FAISS index directory: faiss_index_shopping\n",
      "Creating and saving FAISS Document Index...\n",
      "FAISS Document Index Created and Saved.\n"
     ]
    }
   ],
   "source": [
    "faiss_index_dir = \"faiss_index_shopping\"  # You can change this path as needed\n",
    "\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=google_api_key)\n",
    "if not os.path.exists(faiss_index_dir):\n",
    "    os.makedirs(faiss_index_dir)\n",
    "    print(f\"Created FAISS index directory: {faiss_index_dir}\")\n",
    "\n",
    "faiss_main_file = os.path.join(faiss_index_dir, \"index.faiss\")\n",
    "faiss_pkl_file = os.path.join(faiss_index_dir, \"index.pkl\")\n",
    "\n",
    "if os.path.exists(faiss_main_file) and os.path.exists(faiss_pkl_file):\n",
    "    # If index exists, load it\n",
    "    vectorstore = FAISS.load_local(faiss_index_dir, embedding_model, allow_dangerous_deserialization=True)\n",
    "    print(\"FAISS Document Index Loaded from existing directory.\")\n",
    "else:\n",
    "    # If index does not exist, create it and save it\n",
    "    print(\"Creating and saving FAISS Document Index...\")\n",
    "    vectorstore = FAISS.from_documents(documents, embedding_model)\n",
    "    vectorstore.save_local(faiss_index_dir)\n",
    "    print(\"FAISS Document Index Created and Saved.\")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a08d03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_4072\\3388288799.py:30: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "template = '''\n",
    "Chat History : {chat_history}\n",
    "Context : {context}\n",
    "Question : {question}\n",
    "Instructions:\n",
    "1. For the cloth tracker you have to use the below context:\n",
    "-> Apply the appropriate formula to calculate the CO₂ amount based on the question\n",
    "use this formula: Total = [quantity x average weight] x emission factor,  where user will give quantity and material type. from that get average weight from context.\n",
    "-> Output the CO₂ amount (in kg).\n",
    "-> Also give 2 suggestions to reduce this emission.\n",
    "\n",
    "2. For the Grocery tracker you have to use the below context:\n",
    "-> Apply the appropriate formula to calculate the CO₂ amount based on the question\n",
    "use these formulas:\n",
    "for items with emission factor in kg: Total = quantity x emission factor\n",
    "for items with emission factor in liter: Total = quantity x emission factor in liter x 0.001\n",
    "-> Output the CO₂ amount (in kg) for the kg items and for the Co2 amount (in liter) for the liter items.\n",
    "-> Also give 2 suggestions to reduce this emission.\n",
    "\n",
    "3. For the Electronic tracker you have to use the below context:\n",
    "-> Apply the appropriate formula to calculate the CO₂ amount based on the question\n",
    "use these formulas:\n",
    "for items with emission factor: Total = quantity * weight * emission factor\n",
    "-> Output the CO₂ amount (in kg).\n",
    "-> Also give 2 suggestions to reduce this emission.\n",
    "'''\n",
    "custom_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# --- Memory ---\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd96cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.7)\n",
    "\n",
    "conversational_qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever = retriever,\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": custom_prompt},\n",
    "    return_source_documents=False \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ec61063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RAG Chatbot (with Conversation History) ---\n",
      "Type 'exit', 'quit', or 'bye' to end the conversation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_4072\\795605312.py:13: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = conversational_qa_chain({\"question\": user_query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: For the laptop and Split AC:\n",
      "\n",
      "**CO2 Emission Calculation:**\n",
      "\n",
      "*   **Laptop:** 200 kg\n",
      "*   **Split AC (assuming 1.0 Ton):** 600 kg\n",
      "*   **Total CO2 Emission:** 200 kg (laptop) + 600 kg (Split AC) = **800 kg**\n",
      "\n",
      "**Suggestions to reduce this emission:**\n",
      "\n",
      "1.  **Choose energy-efficient models:** When purchasing new electronics, look for products with high energy efficiency ratings (e.g., Energy Star certified appliances) to reduce their operational carbon footprint.\n",
      "2.  **Extend lifespan and repair:** Instead of frequently upgrading, maintain your current devices well and opt for repairs when possible. Extending the life of electronics reduces the demand for new manufacturing, which is a significant source of emissions.\n",
      "Bot: For the smartphone and 1 T-shirt of nylon:\n",
      "\n",
      "**CO2 Emission Calculation:**\n",
      "\n",
      "*   **Smartphone:** 70 kg\n",
      "*   **1 T-shirt (nylon):** [1 (quantity) x 0.2 kg (average weight)] x 18 (emission factor) = **3.6 kg**\n",
      "*   **Total CO2 Emission:** 70 kg (smartphone) + 3.6 kg (1 T-shirt) = **73.6 kg**\n",
      "\n",
      "**Suggestions to reduce these emissions:**\n",
      "\n",
      "1.  **Extend lifespan and repair:** For electronics like smartphones, maintain your current devices well and opt for repairs when possible. Extending their life reduces the demand for new manufacturing, which is a significant source of emissions. Similarly, for clothing, proper care and timely repairs can significantly extend the lifespan of your garments.\n",
      "2.  **Choose sustainable materials & reduce consumption:** When purchasing new clothes, consider options made from eco-friendly materials or recycled fabrics. For both electronics and clothing, reducing overall consumption and buying only what you need can significantly lower your carbon footprint by decreasing the demand for new production.\n",
      "Bot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "#print(os.getenv(\"GOOGLE_API_KEY\"))\n",
    "print(\"\\n--- RAG Chatbot (with Conversation History) ---\")\n",
    "print(\"Type 'exit', 'quit', or 'bye' to end the conversation.\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_query = input(\"You: \")\n",
    "        if user_query.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            print(\"Bot: Goodbye!\")\n",
    "            break\n",
    "\n",
    " \n",
    "        response = conversational_qa_chain({\"question\": user_query})\n",
    "        print(f\"Bot: {response['answer']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
